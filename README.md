# Sentinel-2 Cloud Cover Query üõ∞Ô∏è

Serverless Abfrage von Wolkenbedeckung f√ºr Schweizer Sentinel-2 Szenen.

## Features

- üõ∞Ô∏è **T√§gliche Updates** via GitHub Actions
- üìä **DuckDB-WASM** im Browser - kein Backend n√∂tig
- üá®üá≠ **Swiss STAC Catalog** Integration
- ‚ö° **Schnelle Queries** √ºber Parquet-Dateien
- üéØ **1km√ó1km Aufl√∂sung** f√ºr pr√§zise Wolkenstatistiken

## Live Demo

üëâ **https://YOUR-GITHUB-USERNAME.github.io/sentinel-cloud-query/**

_(Nach dem Setup verf√ºgbar)_

## Schnellstart

### 1. Repository Setup

```bash
# Fork oder Clone dieses Repo
git clone https://github.com/YOUR-USERNAME/sentinel-cloud-query.git
cd sentinel-cloud-query

# Python Environment erstellen (empfohlen)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Dependencies installieren
pip install -r scripts/requirements.txt
```

### 2. Initial Scrape

**Option A: Lokal ausf√ºhren** (empfohlen f√ºr ersten Test)

```bash
python scripts/initial_scrape.py
```

Dies erstellt:
- `data/scenes.parquet` - Metadaten aller Szenen
- `data/tiles/` - 1km√ó1km Wolken-Tiles

**Option B: Via GitHub Actions** (f√ºr volle Automation)

1. Push zu GitHub
2. Gehe zu: Actions ‚Üí "Initial STAC Scrape" ‚Üí "Run workflow"

‚ö†Ô∏è **Wichtig**: Der Initial Scrape kann 1-3 Stunden dauern (je nach Anzahl Szenen).

### 3. GitHub Pages aktivieren

1. Repository Settings ‚Üí Pages
2. Source: `main` branch, `/docs` folder
3. Save

Website verf√ºgbar unter: `https://YOUR-USERNAME.github.io/sentinel-cloud-query/`

### 4. HTML anpassen

In `docs/index.html` **Zeile 259-260** anpassen:

```javascript
const REPO_OWNER = 'YOUR-GITHUB-USERNAME';  // ‚Üê Hier deinen Username eintragen!
const REPO_NAME = 'sentinel-cloud-query';
```

### 5. Commit & Push

```bash
git add .
git commit -m "Initial setup complete"
git push
```

## Nutzung

### Website

1. √ñffne deine GitHub Pages URL
2. Warte bis "‚úÖ Bereit" angezeigt wird
3. W√§hle Zeitraum und maximale Wolkenbedeckung
4. Klicke "üîç Abfragen"

### Lokale Entwicklung

```bash
# Teste Verarbeitung einzelner Szene
python -c "
from scripts.process_cloudmask import process_cloudmask_cog
tiles, summary = process_cloudmask_cog(
    'https://sys-data.int.bgdi.ch/ch.swisstopo.swisseo_s2-sr_v200/2025-01-18t103351/swisseo_s2-sr_v200_mosaic_2025-01-18t103351_cloudmask_10m.tif',
    'test_scene',
    '2025-01-18'
)
print(f'Processed {len(tiles)} tiles')
print(f'Avg cloud: {summary[\"avg_cloud_pct\"]}%')
"
```

## Automatische Updates

Die GitHub Action l√§uft **t√§glich um 3 Uhr UTC** automatisch und:
1. Holt neue Szenen der letzten 2 Tage aus dem STAC Catalog
2. Verarbeitet Cloud Masks zu 1km√ó1km Tiles
3. Updated die Parquet-Dateien
4. Committed die √Ñnderungen

### Manuelles Update ausl√∂sen

Actions ‚Üí "Daily Cloud Mask Update" ‚Üí "Run workflow"

## Datenstruktur

```
data/
‚îú‚îÄ‚îÄ scenes.parquet              # Scene-Metadaten
‚îÇ   ‚îú‚îÄ‚îÄ scene_id               # z.B. "swisseo_s2-sr_v200_mosaic_2025-01-18t103351"
‚îÇ   ‚îú‚îÄ‚îÄ date                   # Aufnahmedatum
‚îÇ   ‚îú‚îÄ‚îÄ cog_url                # URL zur Cloud Mask
‚îÇ   ‚îú‚îÄ‚îÄ avg_cloud_pct          # Durchschn. Wolkenbedeckung
‚îÇ   ‚îú‚îÄ‚îÄ total_tiles            # Anzahl 1km-Kacheln
‚îÇ   ‚îî‚îÄ‚îÄ bounds_*               # Bounding Box
‚îÇ
‚îî‚îÄ‚îÄ tiles/                      # Partitioniert nach scene_id
    ‚îî‚îÄ‚îÄ scene_id=XXX/
        ‚îî‚îÄ‚îÄ data.parquet       # 1km√ó1km Wolken-Tiles
            ‚îú‚îÄ‚îÄ tile_row       # Kachel-Position
            ‚îú‚îÄ‚îÄ tile_col
            ‚îú‚îÄ‚îÄ cloud_pct      # Wolkenbedeckung dieser Kachel
            ‚îî‚îÄ‚îÄ min_x, max_x, min_y, max_y  # Koordinaten
```

## Technologie Stack

- **Python 3.11+** f√ºr Processing
- **DuckDB** f√ºr Analytics & Parquet-Export
- **Rasterio** f√ºr COG-Verarbeitung
- **pystac-client** f√ºr STAC Catalog Zugriff
- **DuckDB-WASM** f√ºr Browser-Queries
- **GitHub Actions** f√ºr Automation
- **GitHub Pages** f√ºr Hosting

## Konfiguration

### Umgebungsvariablen

```bash
# STAC Endpoint (optional, hat Defaults)
export STAC_URL="https://sys-data.int.bgdi.ch/api/stac/v1"
export COLLECTION_ID="ch.swisstopo.swisseo_s2-sr_v200"
```

### Anpassen der Prozessierung

In `scripts/initial_scrape.py`:

```python
MAX_SCENES = 3000  # Limit f√ºr initiales Scraping
```

In `scripts/process_cloudmask.py`:

```python
tile_size_pixels = 100  # 1km bei 10m Aufl√∂sung
                        # √Ñndern f√ºr andere Aufl√∂sungen
```

## Troubleshooting

### "No scenes found" beim Initial Scrape

- √úberpr√ºfe STAC URL und Collection ID
- Teste manuell: https://sys-data.int.bgdi.ch/api/stac/v1/collections/ch.swisstopo.swisseo_s2-sr_v200

### GitHub Actions schl√§gt fehl

- √úberpr√ºfe Logs unter: Actions ‚Üí Failed workflow ‚Üí View logs
- H√§ufigste Ursache: Dependencies fehlen ‚Üí Check requirements.txt

### Website zeigt "Fehler" beim Laden

- √úberpr√ºfe dass GitHub Pages aktiviert ist
- Check Browser Console f√ºr Fehler (F12)
- Stelle sicher dass `REPO_OWNER` in index.html korrekt ist
- CORS: Parquet-Dateien m√ºssen √∂ffentlich lesbar sein

### DuckDB Memory Error

- GitHub Actions haben 7GB RAM Limit
- Bei zu vielen Szenen: Batch Size reduzieren in initial_scrape.py:
  ```python
  if len(batch_scenes) >= 25:  # Statt 50
  ```

## Performance

- Initial Scrape: ~1-3 Stunden f√ºr 3000 Szenen
- Daily Update: ~5-15 Minuten
- Website Query: <2 Sekunden f√ºr typische Abfragen
- Datengr√∂√üe: ~50-100 MB Parquet f√ºr 3000 Szenen

## Lizenz

MIT License

## Contributing

Pull Requests willkommen! Bitte:
1. Fork das Repo
2. Erstelle Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit deine √Ñnderungen (`git commit -m 'Add AmazingFeature'`)
4. Push zum Branch (`git push origin feature/AmazingFeature`)
5. √ñffne Pull Request

## Support

- üêõ Bugs: [GitHub Issues](https://github.com/YOUR-USERNAME/sentinel-cloud-query/issues)
- üí¨ Fragen: [GitHub Discussions](https://github.com/YOUR-USERNAME/sentinel-cloud-query/discussions)

## Datenquelle

Daten von [swisstopo SwissEO](https://www.swisstopo.admin.ch/de/satellite-images-swisseo):
- **Collection**: ch.swisstopo.swisseo_s2-sr_v200
- **STAC Catalog**: https://sys-data.int.bgdi.ch/
- **License**: Free for use with attribution

---

Made with ‚ù§Ô∏è for Swiss Earth Observation
